{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a392b5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59854c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Utils\n",
    "from utils import set_seed\n",
    "\n",
    "# Dataset\n",
    "# from dataset import HappyWhaleDataset\n",
    "\n",
    "# Model\n",
    "from model import HappyWhaleModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586b0f0",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f841744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights file\n",
    "weights_file = 'output/Loss13.7040_epoch16.bin'\n",
    "\n",
    "# Set usefull directories\n",
    "ROOT_DIR = \"/home/jean/datas/happy-whale-and-dolphin\"\n",
    "TRAIN_DIR = \"/home/jean/datas/happy-whale-and-dolphin/train_images\"\n",
    "TEST_DIR = \"/home/jean/datas/happy-whale-and-dolphin/test_images\"\n",
    "\n",
    "def get_train_file_path(id):\n",
    "    return f\"{TRAIN_DIR}/{id}\"\n",
    "\n",
    "CONFIG = {\"seed\": 2022,\n",
    "          \"img_size\": 448,\n",
    "          \"model_name\": \"tf_efficientnet_b0_ns\",\n",
    "          \"num_classes\": 15587,\n",
    "          \"embedding_size\": 512,\n",
    "          \"train_batch_size\": 64,\n",
    "          \"valid_batch_size\": 64,\n",
    "          \"n_fold\": 5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          # ArcFace Hyperparameters\n",
    "          \"s\": 30.0, \n",
    "          \"m\": 0.30,\n",
    "          \"ls_eps\": 0.0,\n",
    "          \"easy_margin\": False\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "# Set seed\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc796661",
   "metadata": {},
   "source": [
    "### Read and custom the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db5ab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/train...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9   \n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250   \n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063   \n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392   \n",
       "\n",
       "                                           file_path  \n",
       "0  /home/jean/datas/happy-whale-and-dolphin/train...  \n",
       "1  /home/jean/datas/happy-whale-and-dolphin/train...  \n",
       "2  /home/jean/datas/happy-whale-and-dolphin/train...  \n",
       "3  /home/jean/datas/happy-whale-and-dolphin/train...  \n",
       "4  /home/jean/datas/happy-whale-and-dolphin/train...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "df['file_path'] = df['image'].apply(get_train_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291c7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"individual_id\"] = encoder.fit_transform(df[\"individual_id\"])\n",
    "\n",
    "# Create folds\n",
    "skf = StratifiedKFold(n_splits=CONFIG[\"n_fold\"])\n",
    "\n",
    "for fold, (_, val_) in enumerate(skf.split(X=df, y=df.individual_id)):\n",
    "    df.loc[val_, \"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fd3e6",
   "metadata": {},
   "source": [
    "### Set data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94634124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db707f",
   "metadata": {},
   "source": [
    "### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33559a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HappyWhaleModel(\n",
       "  (model): EfficientNet(\n",
       "    (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): SiLU(inplace=True)\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "          (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): SiLU(inplace=True)\n",
       "    (global_pool): Identity()\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (pooling): GeM(p=3.0625, eps=1e-06)\n",
       "  (embedding): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  (fc): ArcMarginProduct()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HappyWhaleModel(CONFIG['model_name'], CONFIG['embedding_size'],CONFIG)\n",
    "model.load_state_dict(torch.load(weights_file))\n",
    "model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b710e71",
   "metadata": {},
   "source": [
    "### Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3a7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    LABELS = []\n",
    "    EMBEDS = []\n",
    "    IDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar: \n",
    "        print\n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "        ids = data['id']\n",
    "\n",
    "        outputs = model.extract(images)\n",
    "        \n",
    "        LABELS.append(labels.cpu().numpy())\n",
    "        EMBEDS.append(outputs.cpu().numpy())\n",
    "        IDS.append(ids)\n",
    "    \n",
    "    EMBEDS = np.vstack(EMBEDS)\n",
    "    LABELS = np.concatenate(LABELS)\n",
    "    IDS = np.concatenate(IDS)\n",
    "    \n",
    "    return EMBEDS, LABELS, IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded6e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.ids = df['image'].values\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['individual_id'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx = self.ids[index]\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'id': idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b53469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"train\"])\n",
    "    valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG[\"train_batch_size\"],\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CONFIG[\"valid_batch_size\"],\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3014b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 637/637 [21:23<00:00,  2.01s/it]\n",
      "100%|█████████████████████████████████████████| 160/160 [06:24<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# compute embeddings\n",
    "train_loader, valid_loader = prepare_loaders(df, fold=0)\n",
    "train_embeds, train_labels, train_ids = get_embeddings(model, train_loader, CONFIG['device'])\n",
    "valid_embeds, valid_labels, valid_ids = get_embeddings(model, valid_loader, CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499b1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = normalize(train_embeds, axis=1, norm='l2')\n",
    "valid_embeds = normalize(valid_embeds, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b00d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encoder.inverse_transform(train_labels)\n",
    "valid_labels = encoder.inverse_transform(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb5c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(CONFIG['embedding_size'])\n",
    "index.add(train_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b518ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(valid_embeds, k=50)\n",
    "\n",
    "allowed_targets = np.unique(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa565989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_individual    1854\n",
       "37c7aba965a5        80\n",
       "114207cab555        34\n",
       "a6e325d8e924        31\n",
       "19fbb960f07d        31\n",
       "                  ... \n",
       "e05030227860         1\n",
       "2faf1bcb2167         1\n",
       "c511fbe2acd1         1\n",
       "b90f72a6be9b         1\n",
       "26145086bca6         1\n",
       "Name: target, Length: 4008, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets_df = pd.DataFrame(np.stack([valid_ids, valid_labels], axis=1), columns=['image','target'])\n",
    "val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), 'target'] = 'new_individual'\n",
    "val_targets_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8b8382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10207it [00:04, 2252.06it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_df = []\n",
    "for i, val_id in tqdm(enumerate(valid_ids)):\n",
    "    targets = train_labels[I[i]]\n",
    "    distances = D[i]\n",
    "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
    "    subset_preds['image'] = val_id\n",
    "    valid_df.append(subset_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b82fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>target</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0594dd3c4e6b</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>06b287d73a9f</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0f529de1b7a0</td>\n",
       "      <td>0.999198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>1394c7e5fb92</td>\n",
       "      <td>0.999350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>164a5c36c8a1</td>\n",
       "      <td>0.999033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image        target  distances\n",
       "0  00021adfb725ed.jpg  0594dd3c4e6b   0.999316\n",
       "1  00021adfb725ed.jpg  06b287d73a9f   0.999054\n",
       "2  00021adfb725ed.jpg  0f529de1b7a0   0.999198\n",
       "3  00021adfb725ed.jpg  1394c7e5fb92   0.999350\n",
       "4  00021adfb725ed.jpg  164a5c36c8a1   0.999033"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.concat(valid_df).reset_index(drop=True)\n",
    "valid_df = valid_df.groupby(['image','target']).distances.max().reset_index()\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4ce156",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid_df.sort_values('distances', ascending=False).reset_index(drop=True)\n",
    "valid_df.to_csv('val_neighbors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ba1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_df, threshold=0.2):\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(test_df.iterrows()):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, 'new_individual']\n",
    "        else:\n",
    "            predictions[row.image] = ['new_individual', row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83bb7270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:18, 20496.34it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2402158.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.0: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:18, 21177.09it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2551478.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.1: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:17, 22100.32it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2590853.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.2: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22688.11it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2294156.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.30000000000000004: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:17, 22403.61it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2210868.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.4: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22625.69it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2173601.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.5: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:17, 21867.18it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2492359.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.6000000000000001: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22600.81it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2704951.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.7000000000000001: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22719.46it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2635674.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.8: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22892.14it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2470355.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.9: 0.20598118937983734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383040it [00:16, 22984.89it/s]\n",
      "100%|████████████████████████████████| 10207/10207 [00:00<00:00, 2543898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 1.0: 0.25046046830606444\n"
     ]
    }
   ],
   "source": [
    "best_th = 0\n",
    "best_cv = 0\n",
    "for th in [0.1*x for x in range(11)]:\n",
    "    all_preds = get_predictions(valid_df, threshold=th)\n",
    "    cv = 0\n",
    "    for i,row in val_targets_df.iterrows():\n",
    "        target = row.target\n",
    "        preds = all_preds[row.image]\n",
    "        val_targets_df.loc[i,th] = map_per_image(target, preds)\n",
    "    cv = val_targets_df[th].mean()\n",
    "    print(f\"CV at threshold {th}: {cv}\")\n",
    "    if cv > best_cv:\n",
    "        best_th = th\n",
    "        best_cv = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7253900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold 1.0\n",
      "Best cv 0.25046046830606444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.30000000000000004</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6000000000000001</th>\n",
       "      <th>0.7000000000000001</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.205981</td>\n",
       "      <td>0.250460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.385362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0.0           0.1           0.2  0.30000000000000004  \\\n",
       "count  10207.000000  10207.000000  10207.000000         10207.000000   \n",
       "mean       0.205981      0.205981      0.205981             0.205981   \n",
       "std        0.319504      0.319504      0.319504             0.319504   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.000000      0.000000      0.000000             0.000000   \n",
       "50%        0.000000      0.000000      0.000000             0.000000   \n",
       "75%        0.500000      0.500000      0.500000             0.500000   \n",
       "max        1.000000      1.000000      1.000000             1.000000   \n",
       "\n",
       "                0.4           0.5  0.6000000000000001  0.7000000000000001  \\\n",
       "count  10207.000000  10207.000000        10207.000000        10207.000000   \n",
       "mean       0.205981      0.205981            0.205981            0.205981   \n",
       "std        0.319504      0.319504            0.319504            0.319504   \n",
       "min        0.000000      0.000000            0.000000            0.000000   \n",
       "25%        0.000000      0.000000            0.000000            0.000000   \n",
       "50%        0.000000      0.000000            0.000000            0.000000   \n",
       "75%        0.500000      0.500000            0.500000            0.500000   \n",
       "max        1.000000      1.000000            1.000000            1.000000   \n",
       "\n",
       "                0.8           0.9           1.0  \n",
       "count  10207.000000  10207.000000  10207.000000  \n",
       "mean       0.205981      0.205981      0.250460  \n",
       "std        0.319504      0.319504      0.385362  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000  \n",
       "75%        0.500000      0.500000      0.500000  \n",
       "max        1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best threshold\", best_th)\n",
    "print(\"Best cv\", best_cv)\n",
    "val_targets_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66db6af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 8353, True: 1854}\n",
      "best_threshold 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_new_individual</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>adjusted_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30000000000000004</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6000000000000001</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7000000000000001</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.176650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.084096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_new_individual       False  True  adjusted_cv\n",
       "0.0                  0.140722   0.5     0.176650\n",
       "0.1                  0.140722   0.5     0.176650\n",
       "0.2                  0.140722   0.5     0.176650\n",
       "0.30000000000000004  0.140722   0.5     0.176650\n",
       "0.4                  0.140722   0.5     0.176650\n",
       "0.5                  0.140722   0.5     0.176650\n",
       "0.6000000000000001   0.140722   0.5     0.176650\n",
       "0.7000000000000001   0.140722   0.5     0.176650\n",
       "0.8                  0.140722   0.5     0.176650\n",
       "0.9                  0.140722   0.5     0.176650\n",
       "1.0                  0.084096   1.0     0.175686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets_df['is_new_individual'] = val_targets_df.target=='new_individual'\n",
    "print(val_targets_df.is_new_individual.value_counts().to_dict())\n",
    "val_scores = val_targets_df.groupby('is_new_individual').mean().T\n",
    "val_scores['adjusted_cv'] = val_scores[True]*0.1+val_scores[False]*0.9\n",
    "best_threshold_adjusted = val_scores['adjusted_cv'].idxmax()\n",
    "print(\"best_threshold\",best_threshold_adjusted)\n",
    "val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e72aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50975, 512) (50975,)\n"
     ]
    }
   ],
   "source": [
    "train_embeds = np.concatenate([train_embeds, valid_embeds])\n",
    "train_labels = np.concatenate([train_labels, valid_labels])\n",
    "print(train_embeds.shape,train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01cacc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(CONFIG['embedding_size'])\n",
    "index.add(train_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22e1ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>file_path</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4780170daf460a.jpg</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/test_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d9ece950295bc7.jpg</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/test_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3491858282dc25.jpg</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/test_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37269dbb569b3e.jpg</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/test_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fe39a9c93ac13e.jpg</td>\n",
       "      <td>/home/jean/datas/happy-whale-and-dolphin/test_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                          file_path  \\\n",
       "0  4780170daf460a.jpg  /home/jean/datas/happy-whale-and-dolphin/test_...   \n",
       "1  d9ece950295bc7.jpg  /home/jean/datas/happy-whale-and-dolphin/test_...   \n",
       "2  3491858282dc25.jpg  /home/jean/datas/happy-whale-and-dolphin/test_...   \n",
       "3  37269dbb569b3e.jpg  /home/jean/datas/happy-whale-and-dolphin/test_...   \n",
       "4  fe39a9c93ac13e.jpg  /home/jean/datas/happy-whale-and-dolphin/test_...   \n",
       "\n",
       "   individual_id  \n",
       "0             -1  \n",
       "1             -1  \n",
       "2             -1  \n",
       "3             -1  \n",
       "4             -1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test[\"image\"] = os.listdir(\"/home/jean/datas/happy-whale-and-dolphin/test_images\")\n",
    "test[\"file_path\"] = test[\"image\"].apply(lambda x: f\"{TEST_DIR}/{x}\")\n",
    "test[\"individual_id\"] = -1  #dummy value\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a5d8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HappyWhaleDataset(test, transforms=data_transforms[\"valid\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                         num_workers=2, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96686f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 437/437 [16:51<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "test_embeds, _, test_ids = get_embeddings(model, test_loader, CONFIG['device'])\n",
    "test_embeds = normalize(test_embeds, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9a9fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(test_embeds, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cea18d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27956it [00:11, 2358.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = []\n",
    "for i, test_id in tqdm(enumerate(test_ids)):\n",
    "    targets = train_labels[I[i]]\n",
    "    distances = D[i]\n",
    "    subset_preds = pd.DataFrame(np.stack([targets, distances], axis=1), columns=['target','distances'])\n",
    "    subset_preds['image'] = test_id\n",
    "    test_df.append(subset_preds)\n",
    "    \n",
    "test_df = pd.concat(test_df).reset_index(drop=True)\n",
    "test_df = test_df.groupby(['image','target']).distances.max().reset_index()\n",
    "test_df = test_df.sort_values('distances', ascending=False).reset_index(drop=True)\n",
    "test_df.to_csv('test_neighbors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad81cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1076896it [00:46, 22992.33it/s]\n",
      "100%|████████████████████████████████| 27956/27956 [00:00<00:00, 2028790.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491c56c1a3baac.jpg</td>\n",
       "      <td>5fc809d9e819 new_individual babd014300b7 6c029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b53f392ea1409b.jpg</td>\n",
       "      <td>29623de1f9a5 new_individual 91970ecd7d55 babd0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6ba6bc50e83c7c.jpg</td>\n",
       "      <td>3ee0ab1d8230 new_individual 86257eaa613b 45fc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b557f16c8366a.jpg</td>\n",
       "      <td>262f464ee602 new_individual 35bc67b3b071 5fc80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1367ddbab5c50.jpg</td>\n",
       "      <td>12f2a4406d7e new_individual 79c94459ece0 84502...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions\n",
       "0  491c56c1a3baac.jpg  5fc809d9e819 new_individual babd014300b7 6c029...\n",
       "1  b53f392ea1409b.jpg  29623de1f9a5 new_individual 91970ecd7d55 babd0...\n",
       "2  6ba6bc50e83c7c.jpg  3ee0ab1d8230 new_individual 86257eaa613b 45fc3...\n",
       "3  1b557f16c8366a.jpg  262f464ee602 new_individual 35bc67b3b071 5fc80...\n",
       "4  f1367ddbab5c50.jpg  12f2a4406d7e new_individual 79c94459ece0 84502..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(test_df, best_threshold_adjusted)\n",
    "\n",
    "predictions = pd.Series(predictions).reset_index()\n",
    "predictions.columns = ['image','predictions']\n",
    "predictions['predictions'] = predictions['predictions'].apply(lambda x: ' '.join(x))\n",
    "predictions.to_csv('submission.csv',index=False)\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
